{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Envis Insight Engine - Training Notebook\n",
        "\n",
        "This notebook trains the Envis Insight Engine model on the labelled dataset.\n",
        "\n",
        "**Requirements:**\n",
        "- Google Colab Pro (for GPU access)\n",
        "- A100 or V100 GPU runtime\n",
        "- `financial_data_8k.csv` uploaded\n",
        "\n",
        "**Expected Training Time:** ~6-8 hours on A100"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup Environment"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU\n",
        "!nvidia-smi"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install torch transformers torch-geometric pandas scikit-learn tensorboard tqdm"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoModel, AutoTokenizer, get_cosine_schedule_with_warmup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, mean_absolute_error\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Upload and Load Data"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload your CSV file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  # Select financial_data_8k.csv"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "df = pd.read_csv('financial_data_8k.csv')\n",
        "\n",
        "print(f\"Total records: {len(df):,}\")\n",
        "print(f\"Columns: {list(df.columns)}\")\n",
        "print(f\"\\nDistress distribution:\")\n",
        "print(df['distress'].value_counts())\n",
        "print(f\"\\nFraming distribution:\")\n",
        "print(df['framing'].value_counts())"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode framing labels\n",
        "FRAMING_CLASSES = ['supportive', 'direct', 'celebratory', 'gentle', 'urgent']\n",
        "framing_to_idx = {f: i for i, f in enumerate(FRAMING_CLASSES)}\n",
        "df['framing_idx'] = df['framing'].map(framing_to_idx)\n",
        "\n",
        "# Split data (80/10/10)\n",
        "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['distress'])\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['distress'])\n",
        "\n",
        "print(f\"Train: {len(train_df):,}\")\n",
        "print(f\"Val: {len(val_df):,}\")\n",
        "print(f\"Test: {len(test_df):,}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Define Dataset and Model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Load FinBERT tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('ProsusAI/finbert')\n",
        "\n",
        "class FinancialDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length=256):\n",
        "        self.data = dataframe.reset_index(drop=True)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        \n",
        "        # Tokenize text\n",
        "        encoding = self.tokenizer(\n",
        "            row['text'],\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            'distress': torch.tensor(row['distress'], dtype=torch.float),\n",
        "            'framing': torch.tensor(row['framing_idx'], dtype=torch.long),\n",
        "            'tension': torch.tensor(row['tension'], dtype=torch.float),\n",
        "        }\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = FinancialDataset(train_df, tokenizer)\n",
        "val_dataset = FinancialDataset(val_df, tokenizer)\n",
        "test_dataset = FinancialDataset(test_df, tokenizer)\n",
        "\n",
        "# Create dataloaders\n",
        "BATCH_SIZE = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Val batches: {len(val_loader)}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adapter module for parameter-efficient fine-tuning\n",
        "class Adapter(nn.Module):\n",
        "    def __init__(self, input_dim, bottleneck_dim=64):\n",
        "        super().__init__()\n",
        "        self.down = nn.Linear(input_dim, bottleneck_dim)\n",
        "        self.up = nn.Linear(bottleneck_dim, input_dim)\n",
        "        self.act = nn.GELU()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return x + self.up(self.act(self.down(x)))\n",
        "\n",
        "\n",
        "class EnvisInsightEngine(nn.Module):\n",
        "    \"\"\"\n",
        "    Simplified Envis Insight Engine for proof-of-concept training.\n",
        "    \n",
        "    Uses FinBERT with adapter fine-tuning for text encoding,\n",
        "    plus multi-task prediction heads.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_framing_classes=5, adapter_dim=64):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Load FinBERT (frozen)\n",
        "        self.bert = AutoModel.from_pretrained('ProsusAI/finbert')\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = False\n",
        "        \n",
        "        # Add adapters to each layer\n",
        "        self.adapters = nn.ModuleList([\n",
        "            Adapter(768, adapter_dim) for _ in range(12)\n",
        "        ])\n",
        "        \n",
        "        # Prediction heads\n",
        "        self.distress_head = nn.Sequential(\n",
        "            nn.Linear(768, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        \n",
        "        self.framing_head = nn.Sequential(\n",
        "            nn.Linear(768, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(256, num_framing_classes)\n",
        "        )\n",
        "        \n",
        "        self.tension_head = nn.Sequential(\n",
        "            nn.Linear(768, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        \n",
        "        # Uncertainty weights for multi-task learning\n",
        "        self.log_vars = nn.Parameter(torch.zeros(3))\n",
        "    \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Get BERT outputs\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        \n",
        "        # Apply adapters to CLS token\n",
        "        hidden = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
        "        for adapter in self.adapters:\n",
        "            hidden = adapter(hidden)\n",
        "        \n",
        "        # Predictions\n",
        "        distress = self.distress_head(hidden).squeeze(-1)\n",
        "        framing = self.framing_head(hidden)\n",
        "        tension = self.tension_head(hidden).squeeze(-1)\n",
        "        \n",
        "        return {\n",
        "            'distress': distress,\n",
        "            'framing': framing,\n",
        "            'tension': tension,\n",
        "            'log_vars': self.log_vars\n",
        "        }\n",
        "\n",
        "\n",
        "# Initialize model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = EnvisInsightEngine().to(device)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"Trainable %: {100*trainable_params/total_params:.2f}%\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Training Loop"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(predictions, targets):\n",
        "    \"\"\"Uncertainty-weighted multi-task loss.\"\"\"\n",
        "    log_vars = predictions['log_vars']\n",
        "    \n",
        "    # Individual losses\n",
        "    loss_distress = F.binary_cross_entropy(predictions['distress'], targets['distress'])\n",
        "    loss_framing = F.cross_entropy(predictions['framing'], targets['framing'])\n",
        "    loss_tension = F.binary_cross_entropy(predictions['tension'], targets['tension'])\n",
        "    \n",
        "    # Uncertainty weighting\n",
        "    total_loss = (\n",
        "        torch.exp(-log_vars[0]) * loss_distress + log_vars[0] +\n",
        "        torch.exp(-log_vars[1]) * loss_framing + log_vars[1] +\n",
        "        torch.exp(-log_vars[2]) * loss_tension + log_vars[2]\n",
        "    )\n",
        "    \n",
        "    return total_loss, {\n",
        "        'distress': loss_distress.item(),\n",
        "        'framing': loss_framing.item(),\n",
        "        'tension': loss_tension.item()\n",
        "    }\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, device):\n",
        "    \"\"\"Evaluate model on a dataset.\"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    all_distress_pred, all_distress_true = [], []\n",
        "    all_framing_pred, all_framing_true = [], []\n",
        "    all_tension_pred, all_tension_true = [], []\n",
        "    total_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            \n",
        "            predictions = model(input_ids, attention_mask)\n",
        "            \n",
        "            targets = {\n",
        "                'distress': batch['distress'].to(device),\n",
        "                'framing': batch['framing'].to(device),\n",
        "                'tension': batch['tension'].to(device)\n",
        "            }\n",
        "            \n",
        "            loss, _ = compute_loss(predictions, targets)\n",
        "            total_loss += loss.item()\n",
        "            \n",
        "            all_distress_pred.extend(predictions['distress'].cpu().numpy())\n",
        "            all_distress_true.extend(batch['distress'].numpy())\n",
        "            all_framing_pred.extend(predictions['framing'].argmax(dim=1).cpu().numpy())\n",
        "            all_framing_true.extend(batch['framing'].numpy())\n",
        "            all_tension_pred.extend(predictions['tension'].cpu().numpy())\n",
        "            all_tension_true.extend(batch['tension'].numpy())\n",
        "    \n",
        "    metrics = {\n",
        "        'loss': total_loss / len(dataloader),\n",
        "        'distress_auc': roc_auc_score(all_distress_true, all_distress_pred),\n",
        "        'framing_acc': accuracy_score(all_framing_true, all_framing_pred),\n",
        "        'tension_auc': roc_auc_score(all_tension_true, all_tension_pred),\n",
        "    }\n",
        "    \n",
        "    return metrics"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training configuration\n",
        "EPOCHS = 20\n",
        "LEARNING_RATE = 1e-4\n",
        "WEIGHT_DECAY = 0.01\n",
        "WARMUP_STEPS = 500\n",
        "PATIENCE = 3\n",
        "\n",
        "# Optimizer (only trainable parameters)\n",
        "optimizer = torch.optim.AdamW(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    lr=LEARNING_RATE,\n",
        "    weight_decay=WEIGHT_DECAY\n",
        ")\n",
        "\n",
        "# Scheduler\n",
        "total_steps = len(train_loader) * EPOCHS\n",
        "scheduler = get_cosine_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=WARMUP_STEPS,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "# Training loop\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "training_log = []\n",
        "\n",
        "print(\"Starting training...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "    \n",
        "    for batch in progress_bar:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        \n",
        "        targets = {\n",
        "            'distress': batch['distress'].to(device),\n",
        "            'framing': batch['framing'].to(device),\n",
        "            'tension': batch['tension'].to(device)\n",
        "        }\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(input_ids, attention_mask)\n",
        "        loss, task_losses = compute_loss(predictions, targets)\n",
        "        \n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        progress_bar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
        "    \n",
        "    # Validation\n",
        "    val_metrics = evaluate(model, val_loader, device)\n",
        "    \n",
        "    # Log\n",
        "    log_entry = {\n",
        "        'epoch': epoch + 1,\n",
        "        'train_loss': epoch_loss / len(train_loader),\n",
        "        'val_loss': val_metrics['loss'],\n",
        "        'distress_auc': val_metrics['distress_auc'],\n",
        "        'framing_acc': val_metrics['framing_acc'],\n",
        "        'tension_auc': val_metrics['tension_auc'],\n",
        "    }\n",
        "    training_log.append(log_entry)\n",
        "    \n",
        "    print(f\"\\nEpoch {epoch+1}: \"\n",
        "          f\"train_loss={log_entry['train_loss']:.4f}, \"\n",
        "          f\"val_loss={val_metrics['loss']:.4f}, \"\n",
        "          f\"distress_auc={val_metrics['distress_auc']:.4f}, \"\n",
        "          f\"framing_acc={val_metrics['framing_acc']:.4f}\")\n",
        "    \n",
        "    # Early stopping\n",
        "    if val_metrics['loss'] < best_val_loss:\n",
        "        best_val_loss = val_metrics['loss']\n",
        "        patience_counter = 0\n",
        "        # Save best model\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_loss': val_metrics['loss'],\n",
        "            'metrics': val_metrics,\n",
        "        }, 'best_model.pt')\n",
        "        print(\"  âœ“ New best model saved!\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(f\"  No improvement. Patience: {patience_counter}/{PATIENCE}\")\n",
        "    \n",
        "    if patience_counter >= PATIENCE:\n",
        "        print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n",
        "        break\n",
        "\n",
        "print(\"\\nTraining complete!\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Evaluate on Test Set"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Load best model\n",
        "checkpoint = torch.load('best_model.pt')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "print(f\"Loaded best model from epoch {checkpoint['epoch']+1}\")\n",
        "\n",
        "# Evaluate on test set\n",
        "test_metrics = evaluate(model, test_loader, device)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"FINAL TEST SET RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Distress AUC:    {test_metrics['distress_auc']:.4f}\")\n",
        "print(f\"Framing Acc:     {test_metrics['framing_acc']:.4f}\")\n",
        "print(f\"Tension AUC:     {test_metrics['tension_auc']:.4f}\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Save Final Model and Logs"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Save training log\n",
        "log_df = pd.DataFrame(training_log)\n",
        "log_df.to_csv('training_log.csv', index=False)\n",
        "\n",
        "# Save final results\n",
        "results = {\n",
        "    'run_id': f'envis_poc_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
        "    'training_completed': datetime.now().isoformat(),\n",
        "    'epochs_trained': len(training_log),\n",
        "    'best_epoch': checkpoint['epoch'] + 1,\n",
        "    'test_metrics': test_metrics,\n",
        "    'training_log': training_log\n",
        "}\n",
        "\n",
        "with open('training_results.json', 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(\"Saved:\")\n",
        "print(\"  - best_model.pt (model weights)\")\n",
        "print(\"  - training_log.csv\")\n",
        "print(\"  - training_results.json\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download files\n",
        "from google.colab import files\n",
        "\n",
        "files.download('best_model.pt')\n",
        "files.download('training_log.csv')\n",
        "files.download('training_results.json')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}
